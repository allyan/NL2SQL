{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/allyan/NL2SQL/blob/main/Copy_of_bert_weaviate_information_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PFWow-Qy2oTl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7529f50-78d6-49f1-9d42-9c7df90e9e8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting weaviate-client==3.2.2\n",
            "  Downloading weaviate_client-3.2.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.59.0 in /usr/local/lib/python3.8/dist-packages (from weaviate-client==3.2.2) (4.64.1)\n",
            "Requirement already satisfied: requests<2.27.0,>=2.23.0 in /usr/local/lib/python3.8/dist-packages (from weaviate-client==3.2.2) (2.25.1)\n",
            "Collecting validators<0.19.0,>=0.18.2\n",
            "  Downloading validators-0.18.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<2.27.0,>=2.23.0->weaviate-client==3.2.2) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<2.27.0,>=2.23.0->weaviate-client==3.2.2) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<2.27.0,>=2.23.0->weaviate-client==3.2.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<2.27.0,>=2.23.0->weaviate-client==3.2.2) (4.0.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from validators<0.19.0,>=0.18.2->weaviate-client==3.2.2) (1.15.0)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from validators<0.19.0,>=0.18.2->weaviate-client==3.2.2) (4.4.2)\n",
            "Installing collected packages: validators, weaviate-client\n",
            "Successfully installed validators-0.18.2 weaviate-client-3.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip3 install transformers\n",
        "!pip3 install nltk\n",
        "!pip3 install torch\n",
        "!pip3 install weaviate-client==3.2.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0NnIU2E2oTm"
      },
      "source": [
        "## Import the BERT transformer model and pytorch\n",
        "\n",
        "We are using the `bert-base-uncased` model in this example, but any model will work. Feel free to adjust accordingly.\n",
        "\n",
        "## Initialize Weaviate Client\n",
        "This assumes you have Weaviate running locally on `:8080`. Adjust URL accordingly. You could also enter the WCS URL here, for example, if you are running a WCS cloud instance instead of running Weaviate locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "dsr92e332oTn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b850949b-296b-42f2-8a9c-d5f3a63bde56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import weaviate\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# udpated to use different model if desired\n",
        "MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "# model.to('cuda') # remove if working without GPUs\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# initialize nltk (for tokenizing sentences)\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# initialize weaviate client for importing and searching\n",
        "client = weaviate.Client(\"http://64.71.146.93:8080\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKdQOPLV2oTp"
      },
      "source": [
        "## Load dataset from disk\n",
        "Create some helper functions to create the dataset (20-newsgroup text posts) from disk. These methods are specific to the structure of your dataset, adjust accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "FBCEuHrR2oTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2134afc-8d6b-44a4-b2d4-580235dd5393"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "429\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "questions = []\n",
        "sqls = []\n",
        "\n",
        "def read_dataset():\n",
        "    file_data = open(\"sample_data/dataset.txt\", 'r', encoding=\"utf8\")\n",
        "    for line in file_data:\n",
        "        if line.split(\" : \")[0] not in questions and line.split(\" : \")[1] not in sqls:\n",
        "            questions.append(line.split(\" : \")[0])\n",
        "            sqls.append(line.split(\" : \")[1])\n",
        "\n",
        "read_dataset()\n",
        "print(len(questions))\n",
        "       \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adT5ZpgI2oTq"
      },
      "source": [
        "## Vectorize Dataset using BERT\n",
        "\n",
        "The following is a helper function to vectorize all posts (using our BERT transformer) which are entered as an array. The return array contains all the vectors in the same order. BERT is optimized to run on GPUs, if you're using CPUs this might take a while. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "x3juK5Md2oTq"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def text2vec(text):\n",
        "    tokens_pt = tokenizer(text, padding=True, truncation=True, max_length=500, add_special_tokens = True, return_tensors=\"pt\")\n",
        "    outputs = model(**tokens_pt)\n",
        "    # tokens_pt.to('cuda') # remove if working without GPUs\n",
        "    return outputs[0].mean(0).mean(0).detach()\n",
        "\n",
        "def vectorize_questions(posts=[]):\n",
        "    post_vectors=[]\n",
        "    before=time.time()\n",
        "    for i, post in enumerate(posts):\n",
        "        vec=text2vec(sent_tokenize(post))\n",
        "        post_vectors += [vec]\n",
        "        if i % 500 == 0 and i != 0:\n",
        "            print(\"So far {} objects vectorized in {}s\".format(i, time.time()-before))\n",
        "    after=time.time()\n",
        "    \n",
        "    print(\"Vectorized {} items in {}s\".format(len(posts), after-before))\n",
        "    \n",
        "    return post_vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRePPJ342oTr"
      },
      "source": [
        "### Run everything we have so far\n",
        "\n",
        "It is now time to run the functions we defined before. Let's load 50 random posts from disk, then vectorize them using BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ_VEHpQ2oTr"
      },
      "source": [
        "## Initialize Weaviate\n",
        "\n",
        "Now that we have vectors we can import both the posts and the vectors into Weaviate, so we can then search through them.\n",
        "\n",
        "### Init a simple schema\n",
        "Our schema is very simple, we just have one object class, the \"Post\". A post class has just a single property, which we call \"content\" and is of type \"text\".\n",
        "\n",
        "Each class in schema creates one index, so by running the below we tell weaviate to create one brand new vector index waiting for us to import data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "MUCL0-hi2oTr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f0cdf7a-76f0-43af-82fb-16ec8891cca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"classes\": [\n",
            "        {\n",
            "            \"class\": \"Weak_Sim_Intent\",\n",
            "            \"description\": \"Add weak intent similarity\",\n",
            "            \"invertedIndexConfig\": {\n",
            "                \"bm25\": {\n",
            "                    \"b\": 0.75,\n",
            "                    \"k1\": 1.2\n",
            "                },\n",
            "                \"cleanupIntervalSeconds\": 60,\n",
            "                \"stopwords\": {\n",
            "                    \"additions\": null,\n",
            "                    \"preset\": \"en\",\n",
            "                    \"removals\": null\n",
            "                }\n",
            "            },\n",
            "            \"properties\": [\n",
            "                {\n",
            "                    \"dataType\": [\n",
            "                        \"int\"\n",
            "                    ],\n",
            "                    \"description\": \"Frame_nmb\",\n",
            "                    \"name\": \"frame_nmbr\"\n",
            "                },\n",
            "                {\n",
            "                    \"dataType\": [\n",
            "                        \"string\"\n",
            "                    ],\n",
            "                    \"description\": \"movie_id\",\n",
            "                    \"name\": \"movie_id\",\n",
            "                    \"tokenization\": \"word\"\n",
            "                },\n",
            "                {\n",
            "                    \"dataType\": [\n",
            "                        \"string\"\n",
            "                    ],\n",
            "                    \"description\": \"url\",\n",
            "                    \"name\": \"url\",\n",
            "                    \"tokenization\": \"word\"\n",
            "                },\n",
            "                {\n",
            "                    \"dataType\": [\n",
            "                        \"string\"\n",
            "                    ],\n",
            "                    \"description\": \"caption\",\n",
            "                    \"name\": \"caption\",\n",
            "                    \"tokenization\": \"word\"\n",
            "                },\n",
            "                {\n",
            "                    \"dataType\": [\n",
            "                        \"string\"\n",
            "                    ],\n",
            "                    \"description\": \"intent\",\n",
            "                    \"name\": \"intent\",\n",
            "                    \"tokenization\": \"word\"\n",
            "                }\n",
            "            ],\n",
            "            \"shardingConfig\": {\n",
            "                \"virtualPerPhysical\": 128,\n",
            "                \"desiredCount\": 1,\n",
            "                \"actualCount\": 1,\n",
            "                \"desiredVirtualCount\": 128,\n",
            "                \"actualVirtualCount\": 128,\n",
            "                \"key\": \"_id\",\n",
            "                \"strategy\": \"hash\",\n",
            "                \"function\": \"murmur3\"\n",
            "            },\n",
            "            \"vectorIndexConfig\": {\n",
            "                \"skip\": false,\n",
            "                \"cleanupIntervalSeconds\": 300,\n",
            "                \"maxConnections\": 64,\n",
            "                \"efConstruction\": 128,\n",
            "                \"ef\": -1,\n",
            "                \"dynamicEfMin\": 100,\n",
            "                \"dynamicEfMax\": 500,\n",
            "                \"dynamicEfFactor\": 8,\n",
            "                \"vectorCacheMaxObjects\": 1000000000000,\n",
            "                \"flatSearchCutoff\": 40000,\n",
            "                \"distance\": \"cosine\"\n",
            "            },\n",
            "            \"vectorIndexType\": \"hnsw\",\n",
            "            \"vectorizer\": \"none\"\n",
            "        },\n",
            "        {\n",
            "            \"class\": \"Weak_Sim_Conclusion\",\n",
            "            \"description\": \"Add weak conclusion similarity\",\n",
            "            \"invertedIndexConfig\": {\n",
            "                \"bm25\": {\n",
            "                    \"b\": 0.75,\n",
            "                    \"k1\": 1.2\n",
            "                },\n",
            "                \"cleanupIntervalSeconds\": 60,\n",
            "                \"stopwords\": {\n",
            "                    \"additions\": null,\n",
            "                    \"preset\": \"en\",\n",
            "                    \"removals\": null\n",
            "                }\n",
            "            },\n",
            "            \"properties\": [\n",
            "                {\n",
            "                    \"dataType\": [\n",
            "                        \"int\"\n",
            "                    ],\n",
            "                    \"description\": \"Frame_nmb\",\n",
            "                    \"name\": \"frame_nmbr\"\n",
            "                },\n",
            "                {\n",
            "                    \"dataType\": [\n",
            "                        \"string\"\n",
            "                    ],\n",
            "                    \"description\": \"movie_id\",\n",
            "                    \"name\": \"movie_id\",\n",
            "                    \"tokenization\": \"word\"\n",
            "                },\n",
            "                {\n",
            "                    \"dataType\": [\n",
            "                        \"string\"\n",
            "                    ],\n",
            "                    \"description\": \"url\",\n",
            "                    \"name\": \"url\",\n",
            "                    \"tokenization\": \"word\"\n",
            "                },\n",
            "                {\n",
            "                    \"dataType\": [\n",
            "                        \"string\"\n",
            "                    ],\n",
            "                    \"description\": \"caption\",\n",
            "                    \"name\": \"caption\",\n",
            "                    \"tokenization\": \"word\"\n",
            "                },\n",
            "                {\n",
            "                    \"dataType\": [\n",
            "                        \"string\"\n",
            "                    ],\n",
            "                    \"description\": \"conclusion\",\n",
            "                    \"name\": \"conclusion\",\n",
            "                    \"tokenization\": \"word\"\n",
            "                }\n",
            "            ],\n",
            "            \"shardingConfig\": {\n",
            "                \"virtualPerPhysical\": 128,\n",
            "                \"desiredCount\": 1,\n",
            "                \"actualCount\": 1,\n",
            "                \"desiredVirtualCount\": 128,\n",
            "                \"actualVirtualCount\": 128,\n",
            "                \"key\": \"_id\",\n",
            "                \"strategy\": \"hash\",\n",
            "                \"function\": \"murmur3\"\n",
            "            },\n",
            "            \"vectorIndexConfig\": {\n",
            "                \"skip\": false,\n",
            "                \"cleanupIntervalSeconds\": 300,\n",
            "                \"maxConnections\": 64,\n",
            "                \"efConstruction\": 128,\n",
            "                \"ef\": -1,\n",
            "                \"dynamicEfMin\": 100,\n",
            "                \"dynamicEfMax\": 500,\n",
            "                \"dynamicEfFactor\": 8,\n",
            "                \"vectorCacheMaxObjects\": 1000000000000,\n",
            "                \"flatSearchCutoff\": 40000,\n",
            "                \"distance\": \"cosine\"\n",
            "            },\n",
            "            \"vectorIndexType\": \"hnsw\",\n",
            "            \"vectorizer\": \"none\"\n",
            "        },\n",
            "        {\n",
            "            \"class\": \"SQL_Questions\",\n",
            "            \"description\": \"SQL to Text mapping\",\n",
            "            \"invertedIndexConfig\": {\n",
            "                \"bm25\": {\n",
            "                    \"b\": 0.75,\n",
            "                    \"k1\": 1.2\n",
            "                },\n",
            "                \"cleanupIntervalSeconds\": 60,\n",
            "                \"stopwords\": {\n",
            "                    \"additions\": null,\n",
            "                    \"preset\": \"en\",\n",
            "                    \"removals\": null\n",
            "                }\n",
            "            },\n",
            "            \"properties\": [\n",
            "                {\n",
            "                    \"dataType\": [\n",
            "                        \"string\"\n",
            "                    ],\n",
            "                    \"description\": \"question\",\n",
            "                    \"name\": \"question\",\n",
            "                    \"tokenization\": \"word\"\n",
            "                },\n",
            "                {\n",
            "                    \"dataType\": [\n",
            "                        \"string\"\n",
            "                    ],\n",
            "                    \"description\": \"sql\",\n",
            "                    \"name\": \"sql\",\n",
            "                    \"tokenization\": \"word\"\n",
            "                }\n",
            "            ],\n",
            "            \"shardingConfig\": {\n",
            "                \"virtualPerPhysical\": 128,\n",
            "                \"desiredCount\": 1,\n",
            "                \"actualCount\": 1,\n",
            "                \"desiredVirtualCount\": 128,\n",
            "                \"actualVirtualCount\": 128,\n",
            "                \"key\": \"_id\",\n",
            "                \"strategy\": \"hash\",\n",
            "                \"function\": \"murmur3\"\n",
            "            },\n",
            "            \"vectorIndexConfig\": {\n",
            "                \"skip\": false,\n",
            "                \"cleanupIntervalSeconds\": 300,\n",
            "                \"maxConnections\": 64,\n",
            "                \"efConstruction\": 128,\n",
            "                \"ef\": -1,\n",
            "                \"dynamicEfMin\": 100,\n",
            "                \"dynamicEfMax\": 500,\n",
            "                \"dynamicEfFactor\": 8,\n",
            "                \"vectorCacheMaxObjects\": 1000000000000,\n",
            "                \"flatSearchCutoff\": 40000,\n",
            "                \"distance\": \"cosine\"\n",
            "            },\n",
            "            \"vectorIndexType\": \"hnsw\",\n",
            "            \"vectorizer\": \"none\"\n",
            "        }\n",
            "    ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# import json\n",
        "schema = client.schema.get()\n",
        "print(json.dumps(schema, indent=4))\n",
        "class_obj = {\n",
        "\"class\": \"SQL_Questions\", # <= Change to your class name - it will be your collection\n",
        "\"description\": \"SQL to Text mapping\",\n",
        "\"vectorizer\": \"none\",\n",
        "\"properties\": [\n",
        "    {\n",
        "        \"dataType\": [\n",
        "            \"string\"\n",
        "        ],\n",
        "        \"description\": \"question\",\n",
        "        \"name\": \"question\"\n",
        "    },\n",
        "    {\n",
        "        \"dataType\": [\n",
        "            \"string\"\n",
        "        ],\n",
        "        \"description\": \"sql\",\n",
        "        \"name\": \"sql\"\n",
        "    }\n",
        "]\n",
        "}\n",
        "client.schema.delete_class(\"SQL_Questions\")\n",
        "client.schema.create_class(class_obj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "dQKkwQ312oTr"
      },
      "outputs": [],
      "source": [
        "\n",
        "def import_questions_with_vectors(questions, sqls, vectors, client):\n",
        "    if len(questions) != len(vectors):\n",
        "        raise Exception(\"len of posts ({}) and vectors ({}) does not match\".format(len(questions), len(vectors)))\n",
        "        \n",
        "    for i, question in enumerate(questions):\n",
        "        try:\n",
        "           client.data_object.create(\n",
        "                data_object={\"question\": questions[i], \"sql\": sqls[i]},\n",
        "                class_name='SQLQuestions',\n",
        "                vector=vectors[i]\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "9ihzppoC2oTs"
      },
      "outputs": [],
      "source": [
        "def search(query=\"\", limit=10):\n",
        "    before = time.time()\n",
        "    vec = text2vec(query)\n",
        "    vec_took = time.time() - before\n",
        "\n",
        "    before = time.time()\n",
        "    near_vec = {\"vector\": vec}\n",
        "    res = client \\\n",
        "        .query.get(\"SQLQuestions\", [\"sql\", \"_additional {certainty}\"]) \\\n",
        "        .with_near_vector(near_vec) \\\n",
        "        .with_limit(limit) \\\n",
        "        .do()\n",
        "    search_took = time.time() - before\n",
        "\n",
        "    # print(\"\\nQuery \\\"{}\\\" with {} results took {:.3f}s ({:.3f}s to vectorize and {:.3f}s to search)\" \\\n",
        "          # .format(query, limit, vec_took+search_took, vec_took, search_took))\n",
        "    # print(res)\n",
        "    for post in res[\"data\"][\"Get\"][\"SQLQuestions\"]:\n",
        "        # print(\"{:.4f}: {}\".format(post[\"_additional\"][\"certainty\"], post[\"question\"]))\n",
        "        print(post[\"sql\"])\n",
        "        # print('---')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectors = vectorize_questions(questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tpYMnOhUQgo",
        "outputId": "0d028d55-c891-4477-f7ff-29d9f2ade8c8"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorized 429 items in 39.825576305389404s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "dpCE9CMw2oTs"
      },
      "outputs": [],
      "source": [
        "import_questions_with_vectors(questions, sqls, vectors, client)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "eIvqyIck2oTs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0083f8a-5891-4447-969c-cead5a77dc18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SELECT u.Name, SUM(t.Budget) FROM Users AS u join Tasks AS t on u.Id = t.AssigneeId group by u.Name ORDER BY SUM(t.Budget) DESC limit 1;\n",
            "\n",
            "SELECT u.Name, t.Name FROM Tasks AS t JOIN Users as u ON t.AssigneeId = u.Id WHERE t.Completed = 0 AND t.DueOn > CURRENT_DATE() ORDER BY t.Budget DESC LIMIT 1;\n",
            "\n",
            "SELECT s.Name, SUM(t.Budget) AS budget FROM Sections AS s inner join Tasks AS t on s.Id = t.sectionId group by s.Name ORDER BY budget DESC limit 1;\n",
            "\n",
            "SELECT u.Name, COUNT(*) AS COUNT_overdue FROM Users AS u INNER JOIN Tasks AS t ON u.Id = t.AssigneeId WHERE t.Completed = 0 and t.DueOn < now() and Priority LIKE '%Low%' and budget < 300 group by u.ID ORDER BY COUNT_overdue DESC;\n",
            "\n",
            "SELECT name, budget FROM Tasks ORDER BY budget DESC limit 5;\n",
            "\n",
            "SELECT AVG(budget), month(CompletedAt), year(CompletedAt) FROM Tasks AS t inner join Users AS u on u.Id = t.AssigneeId WHERE u.Name LIKE '%Shir%' group by month(CompletedAt), year(CompletedAt);\n",
            "\n",
            "SELECT Tasks.Budget, Tasks.Name FROM Tasks WHERE Tasks.Budget = (SELECT max(Tasks.Budget) FROM Tasks);\n",
            "\n",
            "SELECT t.Budget FROM Users AS u JOIN Tasks AS t on u.id = t.AssigneeId WHERE t.Priority LIKE '%High%' and t.Completed = 0 ORDER BY t.DueOn ASC limit 1;\n",
            "\n",
            "SELECT SUM(budget) FROM Tasks WHERE month(DueOn) = MONTH(CURRENT_TIMESTAMP);\n",
            "\n",
            "SELECT t.Name, t.Budget FROM Tasks AS t inner join Sections s on t.ProjectId = s.ProjectId WHERE s.Name LIKE '%milestone%' ORDER BY t.Budget DESC limit 1;\n",
            "\n"
          ]
        }
      ],
      "source": [
        "search(\"who has the highest budget in last month\", 10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vca8kWIm2oTt"
      },
      "outputs": [],
      "source": [
        "#\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}