{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFWow-Qy2oTl",
        "outputId": "e731d9db-d00d-4425-d533-b777288dbf08"
      },
      "outputs": [],
      "source": [
        "!pip3 install transformers\n",
        "!pip3 install nltk\n",
        "!pip3 install torch\n",
        "!pip3 install weaviate-client==3.2.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0NnIU2E2oTm"
      },
      "source": [
        "## Import the BERT transformer model and pytorch\n",
        "\n",
        "We are using the `bert-base-uncased` model in this example, but any model will work. Feel free to adjust accordingly.\n",
        "\n",
        "## Initialize Weaviate Client\n",
        "This assumes you have Weaviate running locally on `:8080`. Adjust URL accordingly. You could also enter the WCS URL here, for example, if you are running a WCS cloud instance instead of running Weaviate locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsr92e332oTn",
        "outputId": "704753a3-6479-4620-ef4a-6e87017d78b2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\awx626582\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Downloading: 100%|██████████| 571/571 [00:00<00:00, 141kB/s]\n",
            "C:\\Users\\awx626582\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\awx626582\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Downloading: 100%|██████████| 438M/438M [02:18<00:00, 3.17MB/s] \n",
            "Downloading: 100%|██████████| 363/363 [00:00<00:00, 179kB/s]\n",
            "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 499kB/s] \n",
            "Downloading: 100%|██████████| 466k/466k [00:00<00:00, 482kB/s]  \n",
            "Downloading: 100%|██████████| 239/239 [00:00<00:00, 118kB/s]\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\awx626582\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import weaviate\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# udpated to use different model if desired\n",
        "MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "# model.to('cuda') # remove if working without GPUs\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# initialize nltk (for tokenizing sentences)\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# initialize weaviate client for importing and searching\n",
        "client = weaviate.Client(\"http://64.71.146.93:8080\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKdQOPLV2oTp"
      },
      "source": [
        "## Load dataset from disk\n",
        "Create some helper functions to create the dataset (20-newsgroup text posts) from disk. These methods are specific to the structure of your dataset, adjust accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FBCEuHrR2oTp"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'human_questions_with_sql_queries.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m         questions\u001b[39m.\u001b[39mappend(line\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m : \u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m])\n\u001b[0;32m     11\u001b[0m         sqls\u001b[39m.\u001b[39mappend(line\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m : \u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m1\u001b[39m])\n\u001b[1;32m---> 13\u001b[0m read_dataset()\n",
            "Cell \u001b[1;32mIn[3], line 8\u001b[0m, in \u001b[0;36mread_dataset\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_dataset\u001b[39m():\n\u001b[1;32m----> 8\u001b[0m     file_data \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mhuman_questions_with_sql_queries.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mutf8\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      9\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m file_data:\n\u001b[0;32m     10\u001b[0m         questions\u001b[39m.\u001b[39mappend(line\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m : \u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m])\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'human_questions_with_sql_queries.txt'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "questions = []\n",
        "sqls = []\n",
        "\n",
        "def read_dataset():\n",
        "    file_data = open(\"human_questions_with_sql_queries.txt\", 'r', encoding=\"utf8\")\n",
        "    for line in file_data:\n",
        "        questions.append(line.split(\" : \")[0])\n",
        "        sqls.append(line.split(\" : \")[1])\n",
        "\n",
        "read_dataset()\n",
        "           \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adT5ZpgI2oTq"
      },
      "source": [
        "## Vectorize Dataset using BERT\n",
        "\n",
        "The following is a helper function to vectorize all posts (using our BERT transformer) which are entered as an array. The return array contains all the vectors in the same order. BERT is optimized to run on GPUs, if you're using CPUs this might take a while. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3juK5Md2oTq"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def text2vec(text):\n",
        "    tokens_pt = tokenizer(text, padding=True, truncation=True, max_length=500, add_special_tokens = True, return_tensors=\"pt\")\n",
        "    outputs = model(**tokens_pt)\n",
        "    # tokens_pt.to('cuda') # remove if working without GPUs\n",
        "    return outputs[0].mean(0).mean(0).detach()\n",
        "\n",
        "def vectorize_questions(posts=[]):\n",
        "    post_vectors=[]\n",
        "    before=time.time()\n",
        "    for i, post in enumerate(posts):\n",
        "        vec=text2vec(sent_tokenize(post))\n",
        "        post_vectors += [vec]\n",
        "        if i % 500 == 0 and i != 0:\n",
        "            print(\"So far {} objects vectorized in {}s\".format(i, time.time()-before))\n",
        "    after=time.time()\n",
        "    \n",
        "    print(\"Vectorized {} items in {}s\".format(len(posts), after-before))\n",
        "    \n",
        "    return post_vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRePPJ342oTr"
      },
      "source": [
        "### Run everything we have so far\n",
        "\n",
        "It is now time to run the functions we defined before. Let's load 50 random posts from disk, then vectorize them using BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ_VEHpQ2oTr"
      },
      "source": [
        "## Initialize Weaviate\n",
        "\n",
        "Now that we have vectors we can import both the posts and the vectors into Weaviate, so we can then search through them.\n",
        "\n",
        "### Init a simple schema\n",
        "Our schema is very simple, we just have one object class, the \"Post\". A post class has just a single property, which we call \"content\" and is of type \"text\".\n",
        "\n",
        "Each class in schema creates one index, so by running the below we tell weaviate to create one brand new vector index waiting for us to import data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUCL0-hi2oTr",
        "outputId": "665ef057-c799-4170-ddb6-68924ecfb5b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"classes\": [\n",
            "        {\n",
            "            \"class\": \"Weak_Sim_Intent\",\n",
            "            \"description\": \"Add weak intent similarity\",\n",
            "            \"invertedIndexConfig\": {\n",
            "                \"bm25\": {\n",
            "                    \"b\": 0.75,\n",
            "                    \"k1\": 1.2\n",
            "                },\n",
            "                \"cleanupIntervalSeconds\": 60,\n",
            "                \"stopwords\": {\n",
            "                    \"additions\": null,\n",
            "                    \"preset\": \"en\",\n",
            "                    \"removals\": null\n",
            "                }\n",
            "            },\n",
            "            \"properties\": [\n",
            "                {\n",
            "                    \"dataType\": [\n",
            "                        \"int\"\n",
            "                    ],\n",
            "                    \"description\": \"Frame_nmb\",\n",
            "                    \"name\": \"frame_nmbr\"\n",
            "                },\n",
            "                {\n",
            "                    \"dataType\": [\n",
            "                        \"string\"\n",
            "                    ],\n",
            "                    \"description\": \"movie_id\",\n",
            "                    \"name\": \"movie_id\",\n",
            "                    \"tokenization\": \"word\"\n",
            "                },\n",
            "                {\n",
            "                    \"dataType\": [\n",
            "                        \"string\"\n",
            "                    ],\n",
            "                    \"description\": \"url\",\n",
            "                    \"name\": \"url\",\n",
            "                    \"tokenization\": \"word\"\n",
            "                },\n",
            "                {\n",
            "                    \"dataType\": [\n",
            "                        \"string\"\n",
            "                    ],\n",
            "                    \"description\": \"caption\",\n",
            "                    \"name\": \"caption\",\n",
            "                    \"tokenization\": \"word\"\n",
            "                },\n",
            "                {\n",
            "                    \"dataType\": [\n",
            "                        \"string\"\n",
            "                    ],\n",
            "                    \"description\": \"intent\",\n",
            "                    \"name\": \"intent\",\n",
            "                    \"tokenization\": \"word\"\n",
            "                }\n",
            "            ],\n",
            "            \"shardingConfig\": {\n",
            "                \"virtualPerPhysical\": 128,\n",
            "                \"desiredCount\": 1,\n",
            "                \"actualCount\": 1,\n",
            "                \"desiredVirtualCount\": 128,\n",
            "                \"actualVirtualCount\": 128,\n",
            "                \"key\": \"_id\",\n",
            "                \"strategy\": \"hash\",\n",
            "                \"function\": \"murmur3\"\n",
            "            },\n",
            "            \"vectorIndexConfig\": {\n",
            "                \"skip\": false,\n",
            "                \"cleanupIntervalSeconds\": 300,\n",
            "                \"maxConnections\": 64,\n",
            "                \"efConstruction\": 128,\n",
            "                \"ef\": -1,\n",
            "                \"dynamicEfMin\": 100,\n",
            "                \"dynamicEfMax\": 500,\n",
            "                \"dynamicEfFactor\": 8,\n",
            "                \"vectorCacheMaxObjects\": 1000000000000,\n",
            "                \"flatSearchCutoff\": 40000,\n",
            "                \"distance\": \"cosine\"\n",
            "            },\n",
            "            \"vectorIndexType\": \"hnsw\",\n",
            "            \"vectorizer\": \"none\"\n",
            "        },\n",
            "        {\n",
            "            \"class\": \"Weak_Sim_Conclusion\",\n",
            "            \"description\": \"Add weak conclusion similarity\",\n",
            "            \"invertedIndexConfig\": {\n",
            "                \"bm25\": {\n",
            "                    \"b\": 0.75,\n",
            "                    \"k1\": 1.2\n",
            "                },\n",
            "                \"cleanupIntervalSeconds\": 60,\n",
            "                \"stopwords\": {\n",
            "                    \"additions\": null,\n",
            "                    \"preset\": \"en\",\n",
            "                    \"removals\": null\n",
            "                }\n",
            "            },\n",
            "            \"properties\": [\n",
            "                {\n",
            "                    \"dataType\": [\n",
            "                        \"int\"\n",
            "                    ],\n",
            "                    \"description\": \"Frame_nmb\",\n",
            "                    \"name\": \"frame_nmbr\"\n",
            "                },\n",
            "                {\n",
            "                    \"dataType\": [\n",
            "                        \"string\"\n",
            "                    ],\n",
            "                    \"description\": \"movie_id\",\n",
            "                    \"name\": \"movie_id\",\n",
            "                    \"tokenization\": \"word\"\n",
            "                },\n",
            "                {\n",
            "                    \"dataType\": [\n",
            "                        \"string\"\n",
            "                    ],\n",
            "                    \"description\": \"url\",\n",
            "                    \"name\": \"url\",\n",
            "                    \"tokenization\": \"word\"\n",
            "                },\n",
            "                {\n",
            "                    \"dataType\": [\n",
            "                        \"string\"\n",
            "                    ],\n",
            "                    \"description\": \"caption\",\n",
            "                    \"name\": \"caption\",\n",
            "                    \"tokenization\": \"word\"\n",
            "                },\n",
            "                {\n",
            "                    \"dataType\": [\n",
            "                        \"string\"\n",
            "                    ],\n",
            "                    \"description\": \"conclusion\",\n",
            "                    \"name\": \"conclusion\",\n",
            "                    \"tokenization\": \"word\"\n",
            "                }\n",
            "            ],\n",
            "            \"shardingConfig\": {\n",
            "                \"virtualPerPhysical\": 128,\n",
            "                \"desiredCount\": 1,\n",
            "                \"actualCount\": 1,\n",
            "                \"desiredVirtualCount\": 128,\n",
            "                \"actualVirtualCount\": 128,\n",
            "                \"key\": \"_id\",\n",
            "                \"strategy\": \"hash\",\n",
            "                \"function\": \"murmur3\"\n",
            "            },\n",
            "            \"vectorIndexConfig\": {\n",
            "                \"skip\": false,\n",
            "                \"cleanupIntervalSeconds\": 300,\n",
            "                \"maxConnections\": 64,\n",
            "                \"efConstruction\": 128,\n",
            "                \"ef\": -1,\n",
            "                \"dynamicEfMin\": 100,\n",
            "                \"dynamicEfMax\": 500,\n",
            "                \"dynamicEfFactor\": 8,\n",
            "                \"vectorCacheMaxObjects\": 1000000000000,\n",
            "                \"flatSearchCutoff\": 40000,\n",
            "                \"distance\": \"cosine\"\n",
            "            },\n",
            "            \"vectorIndexType\": \"hnsw\",\n",
            "            \"vectorizer\": \"none\"\n",
            "        },\n",
            "        {\n",
            "            \"class\": \"SQLQuestions\",\n",
            "            \"description\": \"Auto generated class\",\n",
            "            \"invertedIndexConfig\": {\n",
            "                \"bm25\": {\n",
            "                    \"b\": 0.75,\n",
            "                    \"k1\": 1.2\n",
            "                },\n",
            "                \"cleanupIntervalSeconds\": 60,\n",
            "                \"stopwords\": {\n",
            "                    \"additions\": null,\n",
            "                    \"preset\": \"en\",\n",
            "                    \"removals\": null\n",
            "                }\n",
            "            },\n",
            "            \"properties\": [\n",
            "                {\n",
            "                    \"dataType\": [\n",
            "                        \"text\"\n",
            "                    ],\n",
            "                    \"description\": \"Auto generated property\",\n",
            "                    \"name\": \"question\",\n",
            "                    \"tokenization\": \"word\"\n",
            "                },\n",
            "                {\n",
            "                    \"dataType\": [\n",
            "                        \"text\"\n",
            "                    ],\n",
            "                    \"description\": \"Auto generated property\",\n",
            "                    \"name\": \"sQL\",\n",
            "                    \"tokenization\": \"word\"\n",
            "                },\n",
            "                {\n",
            "                    \"dataType\": [\n",
            "                        \"text\"\n",
            "                    ],\n",
            "                    \"description\": \"Auto generated property\",\n",
            "                    \"name\": \"sql\",\n",
            "                    \"tokenization\": \"word\"\n",
            "                }\n",
            "            ],\n",
            "            \"shardingConfig\": {\n",
            "                \"virtualPerPhysical\": 128,\n",
            "                \"desiredCount\": 1,\n",
            "                \"actualCount\": 1,\n",
            "                \"desiredVirtualCount\": 128,\n",
            "                \"actualVirtualCount\": 128,\n",
            "                \"key\": \"_id\",\n",
            "                \"strategy\": \"hash\",\n",
            "                \"function\": \"murmur3\"\n",
            "            },\n",
            "            \"vectorIndexConfig\": {\n",
            "                \"skip\": false,\n",
            "                \"cleanupIntervalSeconds\": 300,\n",
            "                \"maxConnections\": 64,\n",
            "                \"efConstruction\": 128,\n",
            "                \"ef\": -1,\n",
            "                \"dynamicEfMin\": 100,\n",
            "                \"dynamicEfMax\": 500,\n",
            "                \"dynamicEfFactor\": 8,\n",
            "                \"vectorCacheMaxObjects\": 1000000000000,\n",
            "                \"flatSearchCutoff\": 40000,\n",
            "                \"distance\": \"cosine\"\n",
            "            },\n",
            "            \"vectorIndexType\": \"hnsw\",\n",
            "            \"vectorizer\": \"none\"\n",
            "        },\n",
            "        {\n",
            "            \"class\": \"SQL_Questions\",\n",
            "            \"description\": \"SQL to Text mapping\",\n",
            "            \"invertedIndexConfig\": {\n",
            "                \"bm25\": {\n",
            "                    \"b\": 0.75,\n",
            "                    \"k1\": 1.2\n",
            "                },\n",
            "                \"cleanupIntervalSeconds\": 60,\n",
            "                \"stopwords\": {\n",
            "                    \"additions\": null,\n",
            "                    \"preset\": \"en\",\n",
            "                    \"removals\": null\n",
            "                }\n",
            "            },\n",
            "            \"properties\": [\n",
            "                {\n",
            "                    \"dataType\": [\n",
            "                        \"string\"\n",
            "                    ],\n",
            "                    \"description\": \"question\",\n",
            "                    \"name\": \"question\",\n",
            "                    \"tokenization\": \"word\"\n",
            "                },\n",
            "                {\n",
            "                    \"dataType\": [\n",
            "                        \"string\"\n",
            "                    ],\n",
            "                    \"description\": \"sql\",\n",
            "                    \"name\": \"sql\",\n",
            "                    \"tokenization\": \"word\"\n",
            "                }\n",
            "            ],\n",
            "            \"shardingConfig\": {\n",
            "                \"virtualPerPhysical\": 128,\n",
            "                \"desiredCount\": 1,\n",
            "                \"actualCount\": 1,\n",
            "                \"desiredVirtualCount\": 128,\n",
            "                \"actualVirtualCount\": 128,\n",
            "                \"key\": \"_id\",\n",
            "                \"strategy\": \"hash\",\n",
            "                \"function\": \"murmur3\"\n",
            "            },\n",
            "            \"vectorIndexConfig\": {\n",
            "                \"skip\": false,\n",
            "                \"cleanupIntervalSeconds\": 300,\n",
            "                \"maxConnections\": 64,\n",
            "                \"efConstruction\": 128,\n",
            "                \"ef\": -1,\n",
            "                \"dynamicEfMin\": 100,\n",
            "                \"dynamicEfMax\": 500,\n",
            "                \"dynamicEfFactor\": 8,\n",
            "                \"vectorCacheMaxObjects\": 1000000000000,\n",
            "                \"flatSearchCutoff\": 40000,\n",
            "                \"distance\": \"cosine\"\n",
            "            },\n",
            "            \"vectorIndexType\": \"hnsw\",\n",
            "            \"vectorizer\": \"none\"\n",
            "        }\n",
            "    ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "schema = client.schema.get()\n",
        "print(json.dumps(schema, indent=4))\n",
        "class_obj = {\n",
        "\"class\": \"SQL_Questions\", # <= Change to your class name - it will be your collection\n",
        "\"description\": \"SQL to Text mapping\",\n",
        "\"vectorizer\": \"none\",\n",
        "\"properties\": [\n",
        "    {\n",
        "        \"dataType\": [\n",
        "            \"string\"\n",
        "        ],\n",
        "        \"description\": \"question\",\n",
        "        \"name\": \"question\"\n",
        "    },\n",
        "    {\n",
        "        \"dataType\": [\n",
        "            \"string\"\n",
        "        ],\n",
        "        \"description\": \"sql\",\n",
        "        \"name\": \"sql\"\n",
        "    }\n",
        "]\n",
        "}\n",
        "client.schema.delete_class(\"SQL_Questions\")\n",
        "client.schema.create_class(class_obj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQKkwQ312oTr"
      },
      "outputs": [],
      "source": [
        "\n",
        "def import_questions_with_vectors(questions, sqls, vectors, client):\n",
        "    if len(questions) != len(vectors):\n",
        "        raise Exception(\"len of posts ({}) and vectors ({}) does not match\".format(len(questions), len(vectors)))\n",
        "        \n",
        "    for i, question in enumerate(questions):\n",
        "        try:\n",
        "           client.data_object.create(\n",
        "                data_object={\"question\": questions[i], \"sql\": sqls[i]},\n",
        "                class_name='SQLQuestions',\n",
        "                vector=vectors[i]\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ihzppoC2oTs"
      },
      "outputs": [],
      "source": [
        "def search(query=\"\", limit=15):\n",
        "    before = time.time()\n",
        "    vec = text2vec(query)\n",
        "    vec_took = time.time() - before\n",
        "\n",
        "    before = time.time()\n",
        "    near_vec = {\"vector\": vec}\n",
        "    res = client \\\n",
        "        .query.get(\"SQLQuestions\", [\"sql\", \"_additional {certainty}\"]) \\\n",
        "        .with_near_vector(near_vec) \\\n",
        "        .with_limit(limit) \\\n",
        "        .do()\n",
        "    search_took = time.time() - before\n",
        "\n",
        "    # print(\"\\nQuery \\\"{}\\\" with {} results took {:.3f}s ({:.3f}s to vectorize and {:.3f}s to search)\" \\\n",
        "          # .format(query, limit, vec_took+search_took, vec_took, search_took))\n",
        "    # print(res)\n",
        "    for post in res[\"data\"][\"Get\"][\"SQLQuestions\"]:\n",
        "        # print(\"{:.4f}: {}\".format(post[\"_additional\"][\"certainty\"], post[\"question\"]))\n",
        "        print(post[\"sql\"])\n",
        "        # print('---')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tpYMnOhUQgo",
        "outputId": "85e4844b-3a3d-4b3e-f49e-90722403e6e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "So far 500 objects vectorized in 63.61125946044922s\n",
            "So far 1000 objects vectorized in 122.78761100769043s\n",
            "So far 1500 objects vectorized in 181.30901956558228s\n",
            "So far 2000 objects vectorized in 239.4066243171692s\n",
            "So far 2500 objects vectorized in 297.61777806282043s\n",
            "So far 3000 objects vectorized in 355.21032333374023s\n",
            "Vectorized 3044 items in 359.9342269897461s\n"
          ]
        }
      ],
      "source": [
        "vectors = vectorize_questions(questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpCE9CMw2oTs"
      },
      "outputs": [],
      "source": [
        "import_questions_with_vectors(questions, sqls, vectors, client)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIvqyIck2oTs",
        "outputId": "93993101-7156-44c8-b021-97331589562f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SELECT COUNT(*), Users.Name FROM Tasks AS t join Users AS u on t.AssigneeId = u.Id WHERE u.Name LIKE '%Aharon%' and t.Completed = 0 group by u.Name;\n",
            "\n",
            "SELECT COUNT(*), Users.Name FROM Tasks AS t join Users AS u on t.AssigneeId = u.Id WHERE u.Name LIKE '%Aharon%' and t.Completed = 0 group by u.Name;\n",
            "\n",
            "SELECT COUNT(*), Users.Name FROM Tasks AS tasks inner join Users AS users on tasks.AssigneeId = users.Id WHERE Users.Name LIKE '%aharon%' and tasks.Completed = 0 group by Users.Name;\n",
            "\n",
            "SELECT COUNT(*), Users.Name FROM Tasks AS tasks inner join Users AS users on tasks.AssigneeId = users.Id WHERE Users.Name LIKE '%aharon%' and tasks.Completed = 0 group by Users.Name;\n",
            "\n",
            "SELECT COUNT(*), Users.Name FROM Tasks AS t join Users AS u on t.AssigneeId = u.Id WHERE u.Name LIKE '%Aharon%' and t.Completed = 0 group by u.Name;\n",
            "\n",
            "SELECT COUNT(*), Users.Name FROM Tasks AS t join Users AS u on t.AssigneeId = u.Id WHERE u.Name LIKE '%Aharon%' and t.Completed = 0 group by u.Name;\n",
            "\n",
            "SELECT t.Name AS Name_of_task, u.Name, s.Name FROM Tasks AS t inner join Users AS u inner join SubTasks AS s on t.AssigneeId = u.Id and t.Id = s.ParentTaskId WHERE t.NumberofSubTasks > 0 and u.Name LIKE '%Aharon%';\n",
            "\n",
            "SELECT t.Name AS Name_of_task, u.Name, s.Name FROM Tasks AS t inner join Users AS u inner join SubTasks AS s on t.AssigneeId = u.Id and t.Id = s.ParentTaskId WHERE t.NumberofSubTasks > 0 and u.Name LIKE '%Aharon%';\n",
            "\n",
            "SELECT t.Name AS Name_of_task, u.Name, s.Name FROM Tasks AS t inner join Users AS u inner join SubTasks AS s on t.AssigneeId = u.Id and t.Id = s.ParentTaskId WHERE t.NumberofSubTasks > 0 and u.Name LIKE '%aharon%';\n",
            "\n",
            "SELECT t.Name AS Name_of_task, u.Name, s.Name FROM Tasks AS t inner join Users AS u inner join SubTasks AS s on t.AssigneeId = u.Id and t.Id = s.ParentTaskId WHERE t.NumberofSubTasks > 0 and u.Name LIKE '%aharon%';\n",
            "\n",
            "SELECT t.name from Tasks as t inner join Users as u on t.createdby = u.id where u.name like '%Aharon%' ORDER BY createdAt DESC limit 1;\n",
            "\n",
            "SELECT t.name from Tasks as t inner join Users as u on t.createdby = u.id where u.name like '%Aharon%' ORDER BY createdAt DESC limit 1;\n",
            "\n",
            "SELECT COUNT(*) FROM Tasks AS t INNER JOIN Users AS u ON t.AssigneeId = u.Id WHERE YEAR(DueOn) = 2022 and u.Name LIKE '%Aharon%' and Completed = 0;\n",
            "\n",
            "SELECT COUNT(*) FROM Tasks AS t INNER JOIN Users AS u ON t.AssigneeId = u.Id WHERE YEAR(DueOn) = 2022 and u.Name LIKE '%Aharon%' and Completed = 0;\n",
            "\n",
            "SELECT COUNT(*) FROM Tasks AS t JOIN Users AS u ON t.AssigneeId = u.Id WHERE t.DueOn like '%2022%' and u.Name LIKE '%Aharon%' and Completed = 0;\n",
            "\n"
          ]
        }
      ],
      "source": [
        "search(\"all incompleted tasks by Aharon\", 15)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vca8kWIm2oTt"
      },
      "outputs": [],
      "source": [
        "#\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "46a30f3b8f1cabb48bf40da55e96b781e7e76d189e7ae418f9058d0d211de54b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
